name: CBB Analytics â€” Backtester & Results Tracker

# â”€â”€ SCHEDULES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Results tracker:  Daily at 8:00 AM UTC (12:00 AM PST / 1:00 AM PDT)
#                   Runs after all games are final. Processes yesterday's results.
#
# Backtester:       Weekly on Mondays at 10:00 AM UTC (2:00 AM PST)
#                   Full historical replay + weight optimization.
#                   Also runs on-demand via workflow_dispatch.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

on:
  schedule:
    # Results tracker â€” every morning after games complete
    - cron: "0 8 * * *"
    # Backtester â€” weekly on Monday mornings
    - cron: "0 10 * * 1"

  workflow_dispatch:
    inputs:
      run_mode:
        description: "What to run"
        required: true
        default: "tracker_only"
        type: choice
        options:
          - tracker_only        # Process yesterday's results + email
          - backtest_only       # Full backtest + weight optimization + email
          - both                # Run both sequentially
          - tracker_summary     # Print season summary, no processing
      date:
        description: "Tracker: target date YYYYMMDD (blank = yesterday)"
        required: false
        default: ""
      backtest_start:
        description: "Backtester: start date YYYYMMDD (blank = full history)"
        required: false
        default: ""
      optimizer_metric:
        description: "Backtester: optimization target"
        required: false
        default: "ats"
        type: choice
        options: [ats, mae, brier]
      min_games:
        description: "Backtester: minimum games per team before predicting"
        required: false
        default: "5"
      dry_run:
        description: "Tracker: compute outcomes but don't write to log"
        required: false
        default: "false"
        type: choice
        options: ["true", "false"]

permissions:
  contents: write
  actions: read

env:
  TZ: "America/Los_Angeles"

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 0 â€” Decide which jobs to run based on trigger
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  setup:
    runs-on: ubuntu-latest
    outputs:
      run_tracker:  ${{ steps.decide.outputs.run_tracker }}
      run_backtest: ${{ steps.decide.outputs.run_backtest }}

    steps:
      - name: Decide which jobs to run
        id: decide
        run: |
          python3 - <<'PY'
          import os

          event    = os.getenv("GITHUB_EVENT_NAME", "schedule")
          mode     = os.getenv("INPUT_MODE", "")
          ref_name = os.getenv("GITHUB_REF_NAME", "")

          # workflow_dispatch: honor explicit mode selection
          if event == "workflow_dispatch":
              tracker  = mode in ("tracker_only",  "both", "tracker_summary")
              backtest = mode in ("backtest_only", "both")

          # Scheduled: determine from cron expression via UTC hour + day
          elif event == "schedule":
              from datetime import datetime, timezone
              now = datetime.now(timezone.utc)
              # 08:00 UTC = tracker run; 10:00 UTC Monday = backtester
              tracker  = (now.hour == 8)
              backtest = (now.hour == 10 and now.weekday() == 0)

              # Fallback: run tracker if nothing else matches (safety net)
              if not tracker and not backtest:
                  tracker = True

          else:
              tracker  = True
              backtest = False

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"run_tracker={'true' if tracker else 'false'}\n")
              f.write(f"run_backtest={'true' if backtest else 'false'}\n")

          print(f"run_tracker={tracker}  run_backtest={backtest}")
          PY
        env:
          INPUT_MODE: ${{ github.event.inputs.run_mode || '' }}

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # JOB 1 â€” RESULTS TRACKER
  # Processes yesterday's predictions vs actual outcomes.
  # Fires daily at 8AM UTC.
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  results_tracker:
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.run_tracker == 'true'

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Check artifact availability
        id: artifact_check
        uses: actions/github-script@v7
        with:
          script: |
            const names = new Set([
              'espn-cbb-csvs',
              'cbb-results-log',
              'cbb-predictions-rolling-latest',
            ]);
            const found = {
              espnCsvs: false,
              resultsLog: false,
              rollingPreds: false,
            };

            const now = new Date();

            for await (const page of github.paginate.iterator(
              github.rest.actions.listArtifactsForRepo,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                per_page: 100,
              }
            )) {
              for (const artifact of page.data.artifacts || []) {
                if (!names.has(artifact.name)) continue;
                // Skip if expired (expires_at is returned by the list API)
                if (artifact.expires_at && new Date(artifact.expires_at) < now) continue;
                if (artifact.name === 'espn-cbb-csvs')                        found.espnCsvs     = true;
                if (artifact.name === 'cbb-results-log')                      found.resultsLog   = true;
                if (artifact.name === 'cbb-predictions-rolling-latest')       found.rollingPreds = true;
              }
            }

            core.setOutput('espn_csvs',           String(found.espnCsvs));
            core.setOutput('results_log',         String(found.resultsLog));
            core.setOutput('rolling_predictions', String(found.rollingPreds));
            core.info(`Artifact availability: ${JSON.stringify(found)}`);

      # â”€â”€ Restore latest CSVs (pipeline data + existing results log) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Download ESPN CSVs
        if: steps.artifact_check.outputs.espn_csvs == 'true'
        uses: actions/download-artifact@v4
        with:
          name: espn-cbb-csvs
          path: data/
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Skip ESPN CSV download (artifact missing)
        if: steps.artifact_check.outputs.espn_csvs != 'true'
        run: echo "[INFO] Artifact espn-cbb-csvs not found; continuing without download."

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: python -m pip install -r requirements.txt

      - name: Run ESPN pipeline (bootstrap data when artifact missing)
        if: steps.artifact_check.outputs.espn_csvs != 'true'
        run: python espn_pipeline.py --days-back 7

      - name: Download previous results log
        if: steps.artifact_check.outputs.results_log == 'true'
        uses: actions/download-artifact@v4
        with:
          name: cbb-results-log
          path: data/
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Skip previous results log download (artifact missing)
        if: steps.artifact_check.outputs.results_log != 'true'
        run: echo "[INFO] Artifact cbb-results-log not found; first run will create fresh outputs."

      - name: Download latest predictions
        if: steps.artifact_check.outputs.rolling_predictions == 'true'
        uses: actions/download-artifact@v4
        with:
          name: cbb-predictions-rolling-latest
          path: data/
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Skip latest predictions download (artifact missing)
        if: steps.artifact_check.outputs.rolling_predictions != 'true'
        run: echo "[INFO] Artifact cbb-predictions-rolling-latest not found; tracker may have no games to process."

      - name: Generate predictions for target date (when artifact missing)
        if: steps.artifact_check.outputs.rolling_predictions != 'true'
        env:
          TARGET_DATE: ${{ github.event.inputs.date || '' }}
        run: |
          python - <<'PY'
          import os, subprocess, sys, pathlib
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo

          tz   = ZoneInfo("America/Los_Angeles")
          date = (os.getenv("TARGET_DATE") or "").strip()
          if not date:
              date = (datetime.now(tz) - timedelta(days=1)).strftime("%Y%m%d")

          data_dir = pathlib.Path("data")
          if not data_dir.exists() or not (data_dir / "games.csv").exists():
              print("[SKIP] data/games.csv not found â€” cannot generate predictions")
              raise SystemExit(0)

          print(f"[INFO] Generating predictions for {date}...")
          result = subprocess.run(
              [sys.executable, "espn_prediction_runner.py", "--date", date],
              check=False,
          )
          if result.returncode != 0:
              print(f"[WARN] Primary prediction generation exited {result.returncode}")

          # Attempt ensemble (optional â€” may fail if profiles/rankings unavailable)
          subprocess.run([sys.executable, "cbb_ensemble.py"], check=False)
          print("[OK] Prediction generation step complete")
          PY

      - name: Debug data folder (post-download)
        run: |
          echo "[INFO] Listing data/ (max depth 3)"
          find data -maxdepth 3 -type f | sed -n '1,200p' || true

      # â”€â”€ Run tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Run results tracker
        id: tracker
        env:
          TARGET_DATE: ${{ github.event.inputs.date || '' }}
          DRY_RUN:     ${{ github.event.inputs.dry_run || 'false' }}
          RUN_MODE:    ${{ github.event.inputs.run_mode || 'tracker_only' }}
        run: |
          python - <<'PY'
          import os, subprocess, sys

          date     = (os.getenv("TARGET_DATE") or "").strip()
          dry_run  = os.getenv("DRY_RUN", "false").lower() == "true"
          run_mode = os.getenv("RUN_MODE", "tracker_only")

          cmd = [sys.executable, "cbb_results_tracker.py"]

          if run_mode == "tracker_summary":
              cmd.append("--summary")
          else:
              if date:
                  cmd += ["--date", date]
              if dry_run:
                  cmd.append("--dry-run")

          print(f"[RUN] {' '.join(cmd)}")
          result = subprocess.run(cmd, check=False, capture_output=False)

          if result.returncode != 0:
              print(f"[WARN] Tracker exited {result.returncode} â€” may be no games yesterday")
          PY

      # â”€â”€ Read alert count for email subject line â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Check for alerts
        id: alerts
        run: |
          python - <<'PY'
          import pandas as pd, pathlib, os

          alerts_path = pathlib.Path("data/results_alerts.csv")
          n_alerts    = 0
          critical    = 0

          if alerts_path.exists() and alerts_path.stat().st_size > 10:
              df = pd.read_csv(alerts_path)
              if not df.empty:
                  n_alerts = len(df)
                  critical = int((df.get("severity", pd.Series()) == "CRITICAL").sum())

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"n_alerts={n_alerts}\n")
              f.write(f"n_critical={critical}\n")
              f.write(f"has_alerts={'true' if n_alerts > 0 else 'false'}\n")

          print(f"Alerts: {n_alerts} total, {critical} critical")
          PY

      # â”€â”€ Build email body â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Build tracker email
        id: tracker_email
        run: |
          python - <<'PY'
          import pandas as pd, pathlib, json, os
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo

          tz       = ZoneInfo("America/Los_Angeles")
          today    = datetime.now(tz=tz).strftime("%Y-%m-%d")
          yesterday= (datetime.now(tz=tz) - timedelta(days=1)).strftime("%B %d, %Y")

          lines = []
          lines.append(f"CBB Results Tracker â€” {yesterday}")
          lines.append("=" * 60)

          # â”€â”€ Daily results table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          log_path = pathlib.Path("data/results_log.csv")
          if log_path.exists():
              df = pd.read_csv(log_path)
              df["game_datetime_utc"] = pd.to_datetime(
                  df["game_datetime_utc"], utc=True, errors="coerce"
              )
              cutoff   = pd.Timestamp.now(tz="UTC") - pd.Timedelta(days=1)
              df_today = df[df["game_datetime_utc"] >= cutoff].copy()

              if not df_today.empty:
                  ats_valid = df_today["primary_ats_correct"].dropna()
                  ens_valid = df_today["ens_ats_correct"].dropna()
                  lines.append(f"\nGames processed:   {len(df_today)}")
                  lines.append(f"Games with lines:  {len(ats_valid)}")
                  if len(ats_valid):
                      lines.append(f"Primary ATS:       {ats_valid.sum():.0f}/{len(ats_valid)} ({ats_valid.mean()*100:.1f}%)")
                  if len(ens_valid):
                      lines.append(f"Ensemble ATS:      {ens_valid.sum():.0f}/{len(ens_valid)} ({ens_valid.mean()*100:.1f}%)")

                  win_v = df_today["primary_wins_game"].dropna()
                  if len(win_v):
                      lines.append(f"Winner picks:      {win_v.sum():.0f}/{len(win_v)} ({win_v.mean()*100:.1f}%)")

                  err_v = pd.to_numeric(df_today["primary_margin_error"], errors="coerce").dropna()
                  if len(err_v):
                      lines.append(f"Margin MAE:        {err_v.abs().mean():.1f} pts")
                      lines.append(f"Margin Bias:       {err_v.mean():+.1f} pts")

                  lines.append("\nGAME RESULTS")
                  lines.append("-" * 60)
                  lines.append(f"{'MATCHUP':<34} {'ACTUAL':>6} {'PRED':>6} {'ERR':>6} {'ATS':>4}")
                  lines.append("-" * 60)

                  for _, row in df_today.sort_values("actual_margin", ascending=False, na_position='last').iterrows():
                      home = str(row.get("home_team",""))[:14]
                      away = str(row.get("away_team",""))[:14]
                      matchup = f"{home} vs {away}"
                      act = f"{row.get('actual_margin', 0):+.0f}"
                      pred_s = row.get("pred_spread")
                      pred = f"{-float(pred_s):+.0f}" if pd.notna(pred_s) else "N/A"
                      err_s = row.get("primary_margin_error")
                      err = f"{float(err_s):+.0f}" if pd.notna(err_s) else "N/A"
                      ats = {1: " âœ“", 0: " âœ—"}.get(row.get("primary_ats_correct"), " -")
                      lines.append(f"{matchup:<34} {act:>6} {pred:>6} {err:>6} {ats:>4}")
              else:
                  lines.append("\nNo games processed yesterday (off day or no data).")
          else:
              lines.append("\nNo results log found yet.")

          # â”€â”€ Season rolling windows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          summary_path = pathlib.Path("data/results_summary.csv")
          if summary_path.exists():
              sdf = pd.read_csv(summary_path)
              sdf = sdf[sdf["source"] == "primary"]
              lines.append("\n\nROLLING PERFORMANCE")
              lines.append("-" * 60)
              lines.append(f"{'WINDOW':<10} {'N':>5} {'ATS%':>6} {'O/U%':>6} {'WIN%':>6} {'MAE':>6} {'BIAS':>6}")
              lines.append("-" * 60)
              for _, row in sdf.iterrows():
                  ats_flag = " âš¡" if float(row.get("ats_pct",0)) > 52.38 else "  "
                  lines.append(
                      f"{str(row.get('window','')):<10} "
                      f"{int(row.get('n_games',0)):>5} "
                      f"{float(row.get('ats_pct',0)):>5.1f}%{ats_flag}"
                      f"{float(row.get('ou_pct',0)):>5.1f}% "
                      f"{float(row.get('win_pct',0)):>5.1f}% "
                      f"{float(row.get('margin_mae',0)):>6.2f} "
                      f"{float(row.get('margin_bias',0)):>+6.2f}"
                  )

          # â”€â”€ Per-model split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          split_path = pathlib.Path("data/results_model_split.csv")
          if split_path.exists():
              mdf = pd.read_csv(split_path)
              lines.append("\n\nPER-MODEL ATS% (Season / L30 / L7)")
              lines.append("-" * 60)
              lines.append(f"{'MODEL':<18} {'SEASON':>8} {'L30':>8} {'L7':>8}")
              lines.append("-" * 60)
              for _, row in mdf.iterrows():
                  sea = float(row.get("SEASON_ats", 0))
                  l30 = float(row.get("L30_ats", 0))
                  l7  = float(row.get("L7_ats", 0))
                  flag = " âš¡" if sea > 52.38 else "  "
                  lines.append(f"{str(row.get('model','')):<18} {sea:>7.1f}%{flag} {l30:>7.1f}% {l7:>7.1f}%")

          # â”€â”€ Alerts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          alerts_path = pathlib.Path("data/results_alerts.csv")
          if alerts_path.exists():
              adf = pd.read_csv(alerts_path)
              if not adf.empty:
                  lines.append(f"\n\nâš ï¸  ACTIVE ALERTS ({len(adf)})")
                  lines.append("-" * 60)
                  for _, row in adf.iterrows():
                      sev  = str(row.get("severity",""))
                      atype= str(row.get("alert_type",""))
                      msg  = str(row.get("message",""))
                      icon = "ğŸš¨" if sev == "CRITICAL" else "âš ï¸ "
                      lines.append(f"{icon} [{atype}]")
                      lines.append(f"   {msg}")

          lines.append("\n" + "=" * 60)
          lines.append("Generated by CBB Analytics Pipeline")

          body = "\n".join(lines)

          # Write to file for the email step to read
          with open("/tmp/tracker_email_body.txt", "w") as f:
              f.write(body)

          print(body)
          PY

      # â”€â”€ Send tracker email â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Email tracker results
        continue-on-error: true
        env:
          GMAIL_ADDRESS:    ${{ secrets.GMAIL_ADDRESS }}
          GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
          NOTIFY_EMAIL:     ${{ secrets.NOTIFY_EMAIL }}
          N_ALERTS:         ${{ steps.alerts.outputs.n_alerts }}
          N_CRITICAL:       ${{ steps.alerts.outputs.n_critical }}
        run: |
          python send_email.py \
            --subject "CBB Results $(date +'%b %d')" \
            --body-file /tmp/tracker_email_body.txt \
            --attachments \
              data/results_log.csv \
              data/results_summary.csv \
              data/results_model_split.csv \
              data/results_alerts.csv \
            --critical-count ${{ steps.alerts.outputs.n_critical || 0 }}

      # â”€â”€ Persist results log as artifact â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Check results files for artifact upload
        id: upload_results
        if: always()
        run: |
          shopt -s nullglob
          files=(
            data/results_log.csv
            data/results_summary.csv
            data/results_model_split.csv
            data/results_alerts.csv
          )

          existing=0
          for f in "${files[@]}"; do
            if [ -s "$f" ]; then
              existing=1
            fi
          done

          echo "has_results_files=${existing}" >> "$GITHUB_OUTPUT"

      - name: Upload results log artifact
        if: always() && steps.upload_results.outputs.has_results_files == '1'
        uses: actions/upload-artifact@v4
        with:
          name: cbb-results-log
          path: |
            data/results_log.csv
            data/results_summary.csv
            data/results_model_split.csv
            data/results_alerts.csv
          retention-days: 90       # Keep longer â€” this is your performance history
          if-no-files-found: warn

      - name: Skip results log upload (no files generated)
        if: always() && steps.upload_results.outputs.has_results_files != '1'
        run: echo "[INFO] No results files found; skipping upload for this run."

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # JOB 2 â€” BACKTESTER
  # Full historical replay + weight optimization.
  # Runs weekly (Monday) or on-demand.
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  backtester:
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.run_backtest == 'true'

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: python -m pip install -r requirements.txt

      # â”€â”€ Generate required game log inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # actions/download-artifact can only pull from the *current* workflow run
      # unless you provide a run-id (workflow_run trigger). For scheduled/dispatch
      # analytics runs, we generate the inputs directly.
      - name: Run ESPN pipeline (required for backtester)
        run: |
          python espn_pipeline.py --days-back 120

      - name: Preflight backtester inputs
        run: |
          echo "[INFO] Checking backtester input files..."
          ls -lah data || true
          if [ -s data/team_game_weighted.csv ]; then
            echo "[OK] data/team_game_weighted.csv present"
          elif [ -s data/team_game_metrics.csv ]; then
            echo "[OK] data/team_game_metrics.csv present (fallback)"
          else
            echo "[FAIL] Missing game log inputs. Expected data/team_game_weighted.csv or data/team_game_metrics.csv"
            exit 1
          fi

      # â”€â”€ Run backtester â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Run backtester
        id: backtest
        env:
          BACKTEST_START:   ${{ github.event.inputs.backtest_start || '' }}
          OPTIMIZER_METRIC: ${{ github.event.inputs.optimizer_metric || 'ats' }}
          MIN_GAMES:        ${{ github.event.inputs.min_games || '5' }}
        run: |
          python - <<'PY'
          import os, subprocess, sys

          start   = (os.getenv("BACKTEST_START") or "").strip()
          metric  = (os.getenv("OPTIMIZER_METRIC") or "ats").strip()
          min_g   = (os.getenv("MIN_GAMES") or "5").strip()

          cmd = [
              sys.executable, "cbb_backtester.py",
              "--optimizer-metric", metric,
              "--min-games", min_g,
          ]
          if start:
              cmd += ["--start-date", start]

          print(f"[RUN] {' '.join(cmd)}")
          subprocess.run(cmd, check=True)
          PY

      # â”€â”€ Build backtest email â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Build backtest email
        id: backtest_email
        run: |
          python - <<'PY'
          import pandas as pd, json, pathlib, os
          from datetime import datetime
          from zoneinfo import ZoneInfo

          tz    = ZoneInfo("America/Los_Angeles")
          today = datetime.now(tz=tz).strftime("%Y-%m-%d")

          lines = []
          lines.append(f"CBB Backtester Report â€” {today}")
          lines.append("=" * 72)

          # â”€â”€ Model accuracy table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          report_files = sorted(pathlib.Path("data").glob("backtest_model_report_*.csv"))
          if report_files:
              rdf = pd.read_csv(report_files[-1])

              lines.append("\nMODEL ACCURACY REPORT")
              lines.append("-" * 72)
              lines.append(
                  f"{'MODEL':<16} {'N':>5} {'MAE':>6} {'RMSE':>6} {'BIAS':>6} "
                  f"{'WIN%':>6} {'ATS%':>6} {'O/U%':>6} {'BRIER':>7} "
                  f"{'EDGE_N':>7} {'EDGE_ATS':>8} {'ROI':>6}"
              )
              lines.append("-" * 72)

              for _, row in rdf.iterrows():
                  ats  = float(row.get("ats_pct", 0))
                  flag = " âš¡" if ats > 52.38 else "  "
                  lines.append(
                      f"{str(row.get('model_name','')):<16} "
                      f"{int(row.get('n_games',0)):>5} "
                      f"{float(row.get('spread_mae',0)):>6.2f} "
                      f"{float(row.get('spread_rmse',0)):>6.2f} "
                      f"{float(row.get('spread_bias',0)):>+6.2f} "
                      f"{float(row.get('win_pct',0)):>5.1f}% "
                      f"{ats:>5.1f}%{flag}"
                      f"{float(row.get('ou_pct',0)):>5.1f}% "
                      f"{float(row.get('brier_score',0)):>7.4f} "
                      f"{int(row.get('n_edge_games',0)):>7} "
                      f"{float(row.get('edge_ats_pct',0)):>7.1f}% "
                      f"{float(row.get('edge_roi_sim',0)):>+5.1f}%"
                  )
          else:
              lines.append("\nNo model report found.")

          # â”€â”€ Calibration curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          calib_files = sorted(pathlib.Path("data").glob("backtest_calibration_*.csv"))
          if calib_files:
              cdf = pd.read_csv(calib_files[-1])
              if not cdf.empty:
                  lines.append("\n\nCONFIDENCE CALIBRATION")
                  lines.append("(well-calibrated: higher confidence â†’ higher ATS%)")
                  lines.append("-" * 45)
                  lines.append(f"{'CONF RANGE':<16} {'N':>5} {'ATS%':>7}")
                  lines.append("-" * 45)
                  for _, row in cdf.iterrows():
                      lo  = float(row.get("conf_bin_lo", 0))
                      hi  = float(row.get("conf_bin_hi", 0))
                      n   = int(row.get("n_games", 0))
                      ats = float(row.get("ats_pct", 0))
                      bar = "â–ˆ" * int(ats / 5)   # ASCII bar, each block = 5%
                      lines.append(f"{lo:.2f} â€“ {hi:.2f}     {n:>5} {ats:>6.1f}%  {bar}")

          # â”€â”€ Optimized weights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          weights_path = pathlib.Path("data/backtest_optimized_weights.json")
          if weights_path.exists():
              with open(weights_path) as f:
                  wdata = json.load(f)

              lines.append("\n\nOPTIMIZED ENSEMBLE WEIGHTS")
              lines.append(f"(Metric: {wdata.get('metric','ats').upper()} | "
                           f"Value: {wdata.get('value',0):.4f} | "
                           f"N games: {wdata.get('n_games_used',0):,})")
              lines.append("-" * 45)
              lines.append(f"{'MODEL':<18} {'DEFAULT':>9} {'OPTIMIZED':>10} {'DELTA':>8}")
              lines.append("-" * 45)

              opt     = wdata.get("weights", {})
              default = wdata.get("default_weights", {})
              for model, opt_w in opt.items():
                  def_w = float(default.get(model, 0))
                  delta = opt_w - def_w
                  flag  = " â–²" if delta > 0.02 else (" â–¼" if delta < -0.02 else "  ")
                  lines.append(f"{model:<18} {def_w:>9.4f} {opt_w:>10.4f} {delta:>+7.4f}{flag}")

              lines.append("\nTO APPLY: Copy 'optimized' values into EnsembleConfig in cbb_ensemble.py")

          lines.append("\n" + "=" * 72)
          lines.append("Generated by CBB Analytics Pipeline")

          body = "\n".join(lines)

          with open("/tmp/backtest_email_body.txt", "w") as f:
              f.write(body)

          print(body)
          PY

      # â”€â”€ Send backtest email with report attachments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Email backtest report
        continue-on-error: true
        env:
          GMAIL_ADDRESS:      ${{ secrets.GMAIL_ADDRESS }}
          GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
          NOTIFY_EMAIL:       ${{ secrets.NOTIFY_EMAIL }}
        run: |
          python send_email.py \
            --subject "CBB Backtester $(date +'%b %d, %Y')" \
            --body-file /tmp/backtest_email_body.txt \
            --attachments \
              data/backtest_optimized_weights.json \
              data/model_report.csv \
              data/backtest_results.csv \
              data/calibration.csv \
            --critical-count 0

      # â”€â”€ Upload backtest artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Check backtest files for artifact upload
        id: upload_backtest
        if: always()
        run: |
          shopt -s nullglob
          files=(
            data/backtest_results_*.csv
            data/backtest_model_report_*.csv
            data/backtest_calibration_*.csv
            data/backtest_optimized_weights.json
          )

          existing=0
          for f in "${files[@]}"; do
            if [ -s "$f" ]; then
              existing=1
            fi
          done

          echo "has_backtest_files=${existing}" >> "$GITHUB_OUTPUT"

      - name: Upload backtest artifacts
        if: always() && steps.upload_backtest.outputs.has_backtest_files == '1'
        uses: actions/upload-artifact@v4
        with:
          name: cbb-backtest-${{ github.run_id }}
          path: |
            data/backtest_results_*.csv
            data/backtest_model_report_*.csv
            data/backtest_calibration_*.csv
          retention-days: 90
          if-no-files-found: warn

      - name: Upload backtest weights artifact
        if: always() && steps.upload_backtest.outputs.has_backtest_files == '1'
        uses: actions/upload-artifact@v4
        with:
          name: cbb-backtest-weights-${{ github.run_id }}
          path: data/backtest_optimized_weights.json
          retention-days: 90
          if-no-files-found: warn

      - name: Skip backtest artifact upload (no files generated)
        if: always() && steps.upload_backtest.outputs.has_backtest_files != '1'
        run: echo "[INFO] No backtest artifacts found; skipping upload for this run."
