name: CBB Predictions â€” Rolling 40 Hours Ahead

on:
  workflow_run:
    workflows: ["Update ESPN CBB Data"]
    types: [completed]

  workflow_dispatch:
    inputs:
      date:
        description: "Target date YYYYMMDD (optional). If blank, predicts rolling 40h ahead from run time (PST)."
        required: false
        default: ""
      game_type:
        description: "Game type for UWS context"
        required: false
        default: "regular"
        type: choice
        options:
          - regular
          - conf_tournament
          - ncaa_r1
          - ncaa_r2
      decay:
        description: "Game weight decay function"
        required: false
        default: "smooth"
        type: choice
        options:
          - smooth
          - plateau
          - simple

permissions:
  contents: write
  actions: read

jobs:
  predict:
    if: >
      github.event_name == 'workflow_dispatch' ||
      github.event.workflow_run.conclusion == 'success'

    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Download ESPN CSVs (from triggering pipeline run)
        if: github.event_name == 'workflow_run'
        uses: actions/download-artifact@v4
        with:
          name: espn-cbb-csvs
          path: data/
          run-id: ${{ github.event.workflow_run.id }}
        continue-on-error: false

      - name: Download ESPN CSVs (manual trigger â€” latest available)
        if: github.event_name == 'workflow_dispatch'
        uses: actions/download-artifact@v4
        with:
          name: espn-cbb-csvs
          path: data/
          github-token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Verify data files exist
        run: |
          python - <<'PY'
          import pathlib, sys
          required = [
              "data/team_game_weighted.csv",
              "data/games.csv",
          ]
          fallbacks = [
              "data/team_game_metrics.csv",
              "data/team_game_logs.csv",
          ]
          ok = True
          for f in required:
              p = pathlib.Path(f)
              if p.exists():
                  import pandas as pd
                  size = p.stat().st_size
                  rows = len(pd.read_csv(p))
                  print(f"[OK]   {f}: {rows} rows ({size:,} bytes)")
              else:
                  fallback_found = any(pathlib.Path(fb).exists() for fb in fallbacks)
                  if fallback_found and "weighted" in f:
                      print(f"[WARN] {f} not found â€” runner will use fallback")
                  else:
                      print(f"[FAIL] {f} missing â€” cannot run predictions")
                      ok = False
          if not ok:
              sys.exit(1)
          PY

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r requirements.txt

      # Rolling window behavior:
      # - If inputs.date provided: run exactly that YYYYMMDD (old behavior)
      # - Else: run for all PST dates that fall in [now, now+40h], then merge outputs,
      #         and FILTER to the true rolling 40-hour window.
      - name: Run predictions (rolling 40 hours ahead if date blank)
        env:
          TZ: "America/Los_Angeles"
          # Guard: inputs.* only exists for workflow_dispatch
          INPUT_DATE: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.date || '' }}
          GAME_TYPE:  ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.game_type || 'regular' }}
          DECAY:      ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.decay || 'smooth' }}
        run: |
          python - <<'PY'
          import os
          import sys
          import subprocess
          from datetime import datetime, timedelta, date
          from zoneinfo import ZoneInfo
          import pathlib

          tz = ZoneInfo("America/Los_Angeles")
          input_date = (os.getenv("INPUT_DATE") or "").strip()
          game_type  = (os.getenv("GAME_TYPE") or "regular").strip()
          decay      = (os.getenv("DECAY") or "smooth").strip()

          def run_for_date(yyyymmdd: str) -> None:
              cmd = [
                  sys.executable, "espn_prediction_runner.py",
                  "--date", yyyymmdd,
                  "--game-type", game_type,
                  "--decay", decay,
              ]
              print(f"\n[RUN] {' '.join(cmd)}")
              subprocess.run(cmd, check=True)

          if input_date:
              # Old behavior: exactly one date
              run_for_date(input_date)
              print("\n[OK] Single-date predictions complete")
              raise SystemExit(0)

          # Rolling 40h window (PST)
          start_pst = datetime.now(tz=tz)
          end_pst   = start_pst + timedelta(hours=40)

          # Unique PST dates in the window
          dates = []
          d = start_pst.date()
          while d <= end_pst.date():
              dates.append(d.strftime("%Y%m%d"))
              d = d + timedelta(days=1)

          print(f"[INFO] Rolling window PST: {start_pst.isoformat()} -> {end_pst.isoformat()}")
          print(f"[INFO] Dates in window: {dates}")

          # Run runner once per date and keep any per-date outputs it produces
          for yyyymmdd in dates:
              run_for_date(yyyymmdd)

          # Merge into predictions_latest.csv
          # We look for common runner outputs:
          # - data/predictions_YYYYMMDD.csv (preferred)
          # - data/predictions_latest.csv after each run (fallback)
          import pandas as pd

          data_dir = pathlib.Path("data")
          per_date_files = []
          for yyyymmdd in dates:
              p = data_dir / f"predictions_{yyyymmdd}.csv"
              if p.exists():
                  per_date_files.append(p)

          dfs = []
          if per_date_files:
              print(f"[INFO] Merging {len(per_date_files)} per-date files")
              for p in per_date_files:
                  try:
                      df = pd.read_csv(p)
                      if not df.empty:
                          df["_source_file"] = p.name
                      dfs.append(df)
                  except Exception as e:
                      print(f"[WARN] Failed to read {p}: {e}")
          else:
              # Fallback: use latest if runner only writes predictions_latest.csv
              latest = data_dir / "predictions_latest.csv"
              if latest.exists():
                  print("[WARN] No per-date files found; using predictions_latest.csv as-is (runner may only write latest).")
                  raise SystemExit(0)
              print("[FAIL] No predictions outputs found to merge.")
              raise SystemExit(1)

          merged = pd.concat([d for d in dfs if d is not None], ignore_index=True)
          out = data_dir / "predictions_latest.csv"

          if merged.empty:
              # Legit: no games in 40h window
              merged.to_csv(out, index=False)
              print("[OK] Merged output empty (no games in window). Wrote predictions_latest.csv.")
              raise SystemExit(0)

          # De-dupe if game_id exists, else de-dupe on teams + datetime if available
          if "game_id" in merged.columns:
              merged = merged.drop_duplicates(subset=["game_id"], keep="last")
          else:
              key_cols = [c for c in ["game_datetime_utc", "home_team", "away_team"] if c in merged.columns]
              if key_cols:
                  merged = merged.drop_duplicates(subset=key_cols, keep="last")

          # â”€â”€ NEW: Filter to TRUE rolling 40-hour window â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          # Assumes game_datetime_utc is ISO-ish. Convert to UTC -> PST for filter.
          if "game_datetime_utc" in merged.columns:
              dt_utc = pd.to_datetime(merged["game_datetime_utc"], utc=True, errors="coerce")
              dt_pst = dt_utc.dt.tz_convert(tz)
              mask = (dt_pst >= start_pst) & (dt_pst <= end_pst)
              before = len(merged)
              merged = merged[mask].copy()
              after = len(merged)
              print(f"[INFO] Window filter applied: {before} -> {after} rows within next 40h")

          # Sort for readability
          sort_cols = [c for c in ["game_datetime_utc", "game_time_local", "home_team"] if c in merged.columns]
          if sort_cols:
              merged = merged.sort_values(sort_cols)

          merged.to_csv(out, index=False)
          print(f"[OK] Wrote merged rolling-window predictions: {out} ({len(merged)} rows)")
          PY

      - name: Validate prediction output
        run: |
          python - <<'PY'
          import pandas as pd, pathlib, sys

          latest = pathlib.Path("data/predictions_latest.csv")
          if not latest.exists():
              print("[FAIL] predictions_latest.csv not found")
              sys.exit(1)

          df = pd.read_csv(latest)
          if df.empty:
              print("[WARN] predictions_latest.csv is empty â€” no games in window?")
              sys.exit(0)

          print(f"[OK]   {len(df)} game predictions written")
          print(f"[OK]   Columns: {len(df.columns)}")

          if "pred_spread" in df.columns:
              spreads = pd.to_numeric(df["pred_spread"], errors="coerce").dropna()
              if (spreads.abs() > 40).any():
                  print(f"[WARN] Outlier spreads detected: {spreads[spreads.abs() > 40].values}")

          if "model_confidence" in df.columns:
              confs = pd.to_numeric(df["model_confidence"], errors="coerce").dropna()
              if not confs.empty:
                  print(f"[OK]   Confidence range: {confs.min():.1%} â€“ {confs.max():.1%}")

          edges = df[df.get("edge_flag", pd.Series(0, index=df.index)) == 1]
          if not edges.empty:
              print(f"\nâš¡ {len(edges)} EDGE ALERT(s) found:")
              for _, row in edges.iterrows():
                  print(f"   {row.get('home_team','?')} vs {row.get('away_team','?')}: "
                        f"pred {row.get('pred_spread', float('nan')):+.1f} vs line {row.get('spread_line', 'N/A')}")

          if "uws_uws_total" in df.columns:
              uws = pd.to_numeric(df["uws_uws_total"], errors="coerce")
              alerts = df[uws >= 55]
              if not alerts.empty:
                  print(f"\nðŸš¨ {len(alerts)} STRONG UPSET ALERT(s):")
                  for _, row in alerts.iterrows():
                      print(f"   {row.get('home_team','?')} vs {row.get('away_team','?')}: "
                            f"UWS {row.get('uws_uws_total', float('nan')):.0f}/70")

          print("\n[OK]   Validation passed")
          PY

      - name: Display prediction summary
        if: always()
        run: |
          python - <<'PY'
          import pandas as pd, pathlib
          p = pathlib.Path("data/predictions_latest.csv")
          if not p.exists():
              print("No predictions file found")
          else:
              df = pd.read_csv(p)
              if df.empty:
                  print("No predictions (empty file)")
              else:
                  cols = ["game_datetime_utc", "home_team", "away_team", "pred_spread", "pred_total",
                          "spread_line", "spread_diff_vs_line", "model_confidence", "edge_flag"]
                  available = [c for c in cols if c in df.columns]
                  pd.set_option("display.max_rows", 150)
                  pd.set_option("display.max_columns", 30)
                  pd.set_option("display.width", 140)
                  print(df[available].to_string(index=False))
          PY

      - name: Upload predictions artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cbb-predictions-${{ github.run_id }}
          path: |
            data/predictions_*.csv
            data/predictions_latest.csv
          retention-days: 30
          if-no-files-found: warn

      # Optional commit block unchanged
      # - name: Commit predictions to repo
      #   run: |
      #     git config user.name  "github-actions[bot]"
      #     git config user.email "github-actions[bot]@users.noreply.github.com"
      #     git add data/predictions_*.csv data/predictions_latest.csv
      #     git diff --staged --quiet || git commit -m "predictions: $(date +'%Y-%m-%d')"
      #     git push
